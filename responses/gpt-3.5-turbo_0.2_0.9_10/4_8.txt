Sure! Here's the code to split the dataset into training and validation sets:

```
from torch.utils.data import DataLoader, random_split

# Define the sizes of the splits
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

# Split the dataset into non-overlapping training and validation sets
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Create data loaders for the training and validation sets
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)
```

This code uses the `random_split` function from PyTorch to split the dataset into non-overlapping training and validation sets. The `train_size` variable is set to 80% of the dataset size, and the `val_size` variable is set to the remaining 20%. The `random_split` function takes the dataset and a list of sizes for each split, and returns two new datasets with the specified sizes.

After splitting the dataset, the code creates data loaders for the training and validation sets using the `DataLoader` class from PyTorch. The `batch_size` parameter is set to 32, and the `shuffle` parameter is set to `True` to ensure that the data is randomly shuffled before each epoch. The resulting data loaders are stored in the `train_loader` and `validation_loader` variables.