Sure, you can use the `random_split` function from PyTorch to split your dataset into training and validation sets. Here is the code:

```python
from torch.utils.data import DataLoader, random_split

# Assuming that dataset is your dataset loaded with CustomDatasetFromCSV
dataset_size = len(dataset)
train_size = int(0.8 * dataset_size)
validation_size = dataset_size - train_size

train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])

# Define your batch size
batch_size = 32

train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
validation_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=True)
```

In this code, we first calculate the size of the dataset and then calculate the sizes of the training and validation sets. We use the `random_split` function to split the dataset into two at the given indices. We then create `DataLoader` objects for the training and validation sets. The `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples. The batch size is set to 32, but you can adjust this according to your needs. The `shuffle=True` parameter ensures that the data is shuffled at every epoch.