Sure, you can use the `random_split` function from PyTorch to split your dataset into training and validation sets. Here's how you can do it:

```python
from torch.utils.data import DataLoader, random_split

# Assuming that dataset is your CustomDatasetFromCSV object
dataset_size = len(dataset)
train_size = int(0.8 * dataset_size)
validation_size = dataset_size - train_size

train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])

# Create data loaders for training and validation datasets
# You can adjust batch size as per your requirements
batch_size = 32

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)
```

In this code, `random_split` function splits the dataset into non-overlapping new datasets of given lengths. The DataLoader class provides an iterator over the dataset, and supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.

Please replace `batch_size` with the appropriate value as per your requirements.